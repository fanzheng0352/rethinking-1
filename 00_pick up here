# Where you left off

2019-04-02 I was trying to plot r-squared of weak and well fitting models in the hopes of better understanding r-squared. 

2019-04-03 I understand R^2 much better after seeing the origins of the simple R^2 formula. R^2 describes explained variance (var - SSE) as a ratio of all variance in the outcome. 

2019-04-03 __ pick up on understanding how *out-of-sample deviance* can provide a useful target for optimizing model performance. You are at 6.2.2 having just covered how models reflect our ignorance.

2019-04-04 __ pick up on 6.3 to see how regularizing priors and information criteria address concerns about overfitting in out-of-sample deviance that were raised at the end of 6.2.


At end of Ch 6. 
__ I would do well to create a brief overview of the justification for using deviance. It was a lengthy argument. There was plenty of math formulas that I did not much more than loosely grasp. It's hard to know when to stop and learn the math or keep going with the book. 
__ I returned to look at 6.3 and recognized almost nothing. I think that my notes on each chapter will be valuable for creating touch points to refresh on. 

__ leaving off I am trying to understand how the entropy in each distribution (modeled and actual?) results in a dviergence value. 




# For after this course
__ Perhaps a course to review modelr will refresh you on EDA for model fit as found in DataCamp. 

__ a review of probability might help a great deal. I would love to fall back on fluency in that as I read Stats books. Perhaps via Khan or [Datacamp](https://www.datacamp.com/courses/foundations-of-probability-in-r). I am having to take author's word for now on probability. 

__ a review of linear formulas on Khan and Datacamp would be very valuable for recognizing future formulas in your stats study. 