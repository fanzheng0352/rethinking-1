---
title: "06.02 Information Theory and Model Performance"
author: "JP"
date: "4/3/2019"
output: html_document
---

## Overview: 
To balance between over and underfitting you must first choose a *criterion of model performance*. You must choose a *target* that you want the model to perform well at. Information theory provides a common and useful target, the *out-of-sample deviance*.

The path to understanding the relevance of out-of-sample deviance is not direct. Here are the steps ahead.  
    1. establish that joint probability, not average probability, is the right way to judge model accuracy.  
    2. use information theory to establish a measurement scale for distance from perfect accuracy.  
    3. establish deviance as an approximation of relative distance from perfect accuracy.  
    4. establish that it is only deviance out-of-sample that is of interest.  

```{r global options, include=FALSE}
knitr::opts_chunk$set(
  fig.align='center', dpi = 300, 
  include=FALSE, echo=FALSE, message=FALSE, warning=FALSE
)
```

```{r packages and parameters}
library(magrittr)
library(modelr)
library(tidyverse)
```

## 6.2.1 "Establish that joint probability, not average probability, is the right way to judge model accuracy."
There are 2 major dimensions to consider when defining a target for model optimization: 
    1. **Cost-benefit analysis**. How high is the cost of an error? How large are the returns for correct predictions? 
    2. **Accuracy in context**. Some prediction tasks are just easier than others. 

Using *rate of correct prediction* as your target can be problematic when it rewards expensive failures. 

### 6.2.1.1 Costs & Benenfits
By assigning rewards and penalties to direction of right and wrong predictions you can optimize for better types of accuracy than rate of correct prediction. 

### 6.2.1.2 Measuring Accuracy
Maximizing joint probability, the probability of a predicition being right given the number of possible scenarios, is different than maximizing the average probability.

How should we describe the distance between a prediction and the observed event/target? The measure of distance should account for the ease of hitting the target. For instance it is easier to hit a target of 2D target (distance and width), than it is to hit a 3D target of distance & width at the right time. 

As more dimensions are added to the target, the potential to miss and the potential to impress have both grown. 

Keep in mind that the best model is not true. The best model provides the right probabilities given our state of ignorance. Probability is in the model not in the world. With complete information we could predict with certainty, but ignorant of many worldly factors we predict from a state of ignorance. A globe toss only seems random. We know that the globe is 70% water and so we use that to inform our model, but we are ignorant of starting point and torque and air resistance so we do not include those factors in the model. The model describes our state of ignorance.

### 6.2.2 Information and uncertainty
Information theory can help us to Measure the distance of a model's accuracy from a target. Information theory frames the question as, "How much is our uncertainty reduced by learning an outcome?" For instance when a predicted event occurs, it is no longer uncertain. The reduction in uncertainty from predicted to occurred is the information gain / the reduction in uncertainty. 
In this case, **information** is defined as the reduction in uncertainty from learning an outcome. 

We function that will input the probabilities of rain and shine and output a measure of uncertainty about the weather on a future day.

A measure of uncertainty should meet 3 criteria
    1. The measure of uncertainty should be continuous
    2. The measure of uncertainty should increase as the number of possible events increases
    3. The measure of uncertainty should be additive
    
The only function that satisfies these three criteria is **Information Entropy**. 
    `n` is the number of different possible events,
    each event `i` has probability `p~i~`, 
    `p` is the list of probabilities, and  
    the unique measure of uncertainty we seek is:
```{r Information Entropy, echo=FALSE, out.width="400px", include=TRUE}
knitr::include_graphics(here::here("static/images/info_entropy.png"))
```

In plainer words: *The uncertainty contained in a probability distribution is the average log-probability of an event*.

So *H(p)* is our measure of uncertainty. 

For instance, in an area where the p~rain~ = .30 and p~shine~ = .70 we can calculate the information entropy as the average log-probability of all possible events.
```{r}
p_rain = .30  
p_shine = .70

p <- c(p_rain, p_shine)

INFO_ENTRO_rain_30 <- -sum(p*log(p)) %>% round(2)
```

The information entropy for this area is about `r INFO_ENTRO_rain_30`.

In contrast consider an area where the p~shine~ is .99:
```{r}
p_rain = .01  
p_shine = .99

p <- c(p_rain, p_shine)

INFO_ENTRO_rain_01 <- -sum(p*log(p)) %>% round(2)
```

The information entropy for this sunny area is about `r INFO_ENTRO_rain_01`.

### 6.2.3 From entropy to accuracy
Information entropy quantifies uncertainty. Uncertainty about weather is very low in Dubai and very high in Ireland. 

We can use information entropy to say how far a model is from its target. 

**Divergence** is the additional uncertainty induced by using probabilities from one distribution to describe another distribution. 

Divergence is equal to the average difference in log probability between the target (p) and the model (q). As q grows more dfferent from p the divergence also grows. 
